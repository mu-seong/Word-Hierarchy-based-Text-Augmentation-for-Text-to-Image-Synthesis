{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#tokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#상위어, 유의어 대체\n",
    "def synonym_replacement(words, n, sentences):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\ttag = pos_tag(tokenizer.tokenize(sentences))\n",
    "\n",
    "\tnoun_list = []\n",
    "\tetc_list = []\n",
    "\n",
    "\tfor t in tag:\n",
    "\t\tif t[1] in ['NN', 'NNS', 'NNP', 'NNPS', 'PRP', 'PRP$']:\n",
    "\t\t\tnoun_list.append(t[0])\n",
    "\t\telse:\n",
    "\t\t\tetc_list.append(t[0])\n",
    "\n",
    "\tif word in noun_list:\n",
    "\t\ti = noun_list.index(word)\n",
    "\t\tnoun_word = noun_list[i]\n",
    "\n",
    "\t\tfor syn in wordnet.synsets(noun_word):\n",
    "\t\t\tfor l in syn.hypernyms(): \n",
    "\t\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\t\tsynonym = \"\".join([char for char in synonym if char in 'qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\t\tsynonym = synonym[:-1]\n",
    "\t\t\t\tsynonyms.add(synonym)\n",
    "\telse:\n",
    "\t\ti = etc_list.index(word)\n",
    "\t\tetc_word = etc_list[i]\n",
    "\t\tfor syn in wordnet.synsets(etc_word):\n",
    "\t\t\tfor l in syn.lemmas(): \n",
    "\t\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\t\t#synonym = \"\".join([char for char in synonym if char in 'qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\t\tsynonyms.add(synonym)\n",
    "\t\t\t\t\n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "\n",
    "def eda(sentence, alpha_sr, num_aug):\n",
    "    \n",
    "    sentences = get_only_chars(sentence)\n",
    "    words = tokenizer.tokenize(sentences)\n",
    "    #words = [word for word in words if word is not '']\n",
    "    num_words = len(words)\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug/4)+1\n",
    "    n_sr = max(1, int(alpha_sr*num_words))\n",
    "\n",
    "    #sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr, sentences)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    augmented_sentences = [get_only_chars(sentences) for sentences in augmented_sentences]\n",
    "    shuffle(augmented_sentences)\n",
    "\n",
    "    #trim so that we have the desired number of augmented sentences\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "    #append the original sentence\n",
    "    augmented_sentences.append(sentences)\n",
    "\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#유의어 대체\n",
    "\"\"\"\n",
    "def synonym_replacement(words, n, sentences):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "\n",
    "def eda(sentence, alpha_sr, num_aug):\n",
    "    \n",
    "    sentences = get_only_chars(sentence)\n",
    "    words = tokenizer.tokenize(sentences)\n",
    "    #words = [word for word in words if word is not '']\n",
    "    num_words = len(words)\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug/4)+1\n",
    "    n_sr = max(1, int(alpha_sr*num_words))\n",
    "\n",
    "    #sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr, sentences)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    augmented_sentences = [get_only_chars(sentences) for sentences in augmented_sentences]\n",
    "    shuffle(augmented_sentences)\n",
    "\n",
    "    #trim so that we have the desired number of augmented sentences\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "    #append the original sentence\n",
    "    augmented_sentences.append(sentences)\n",
    "\n",
    "    return augmented_sentences\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DT'),\n",
       " ('flat', 'JJ'),\n",
       " ('screen', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('keyboard', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('mouse', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('desk', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = 'a flat screen and a keyboard and mouse on the desk'\n",
    "words = tokenizer.tokenize(sentences)\n",
    "tag = pos_tag(words)\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'flat',\n",
       " 'screen',\n",
       " 'and',\n",
       " 'a',\n",
       " 'keyboard',\n",
       " 'and',\n",
       " 'bruise',\n",
       " 'on',\n",
       " 'the',\n",
       " 'desk']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_replacement(words, 1, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DT'),\n",
       " ('compressed', 'JJ'),\n",
       " ('screen', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('keyboard', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('mouse', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('desk', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences = 'a compressed screen and a keyboard and mouse on the desk'\n",
    "new_words = tokenizer.tokenize(new_sentences)\n",
    "new_tag = pos_tag(new_words)\n",
    "new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = words.copy()\n",
    "random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "random.shuffle(random_word_list)\n",
    "random_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flat tire', 'directly', 'mat', 'prostrate', 'matt', 'flatcar', 'two dimensional', 'flatbed', 'matted', 'straight', 'level', 'plane', 'flavourless', 'flat', '2 dimensional', 'categoric', 'savorless', 'vapid', 'unconditional', 'flavorless', 'compressed', 'monotonic', 'bland', 'apartment', 'savourless', 'monotone', 'monotonous', 'insipid', 'categorical', 'matte'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = set()\n",
    "for syn in wordnet.synsets('flat'):\n",
    "    for l in syn.lemmas():\n",
    "        synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonyms.add(synonym)\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'protectivecovering', 'door', 'strainer', 'covering', 'obstruct', 'sift', 'choose', 'display', 'analyze', 'check', 'show', 'partition', 'filmindustry', 'surface', 'protect'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = set()\n",
    "for syn in wordnet.synsets('screen'):\n",
    "    for l in syn.hypernyms():\n",
    "        synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonym = \"\".join([char for char in synonym if char in 'qwertyuiopasdfghjklzxcvbnm'])\n",
    "        synonym = synonym[:-1]\n",
    "        synonyms.add(synonym)\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wordnet.synsets('car'):\n",
    "    for l in syn.hypernyms():\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wordnet.synsets('motor_vehicle'):\n",
    "    for l in syn.lemmas():\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = 'car.n.01'\n",
    "print(wordnet.synset(target_word).hypernyms())\n",
    "print()\n",
    "for i, path in enumerate(wordnet.synset(target_word).hypernym_paths()):\n",
    "    print(path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hypernym_paths(target_word):\n",
    "    syns = wordnet.synset(target_word)\n",
    "    r_paths = []\n",
    "    def hypernym_paths_helper(input_syns, paths):\n",
    "        if len(input_syns.hypernyms())==0:\n",
    "            for path in paths:\n",
    "                r_paths.append(path)\n",
    "        else:\n",
    "            for hypernym in input_syns.hypernyms():\n",
    "                hypernym_paths_helper(hypernym, [path+[hypernym] for path in paths])\n",
    "    hypernym_paths_helper(syns, [[]])\n",
    "    return r_paths\n",
    "for i, path in enumerate(find_hypernym_paths('car.n.01')):\n",
    "    print(\"path {}: {}\".format(i, path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = 'man.n.01'\n",
    "\n",
    "print(wordnet.synset(word).hypernyms())\n",
    "print()\n",
    "for i, path in enumerate(wordnet.synset(word).hypernym_paths()):\n",
    "    print(path)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word = 'sport_utility.n.01'\n",
    "\n",
    "print(wordnet.synset(word).hypernyms())\n",
    "print(wordnet.synset(word).lemmas())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments to be parsed from command line\n",
    "\"\"\"\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"--input\", required=True, type=str, help=\"input file of unaugmented data\")\n",
    "ap.add_argument(\"--output\", required=False, type=str, help=\"output file of unaugmented data\")\n",
    "ap.add_argument(\"--num_aug\", required=False, type=int, help=\"number of augmented sentences per original sentence\")\n",
    "ap.add_argument(\"--alpha\", required=False, type=float, help=\"percent of words in each sentence to be changed\")\n",
    "args = ap.parse_args()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = 'phone'\n",
    "pos = []\n",
    "for syn in wordnet.synsets(word):\n",
    "    pos.add(syn.pos())\n",
    "    if pos[:1] == 'v':\n",
    "        print('verb')\n",
    "    elif pos[:1] == 'n':\n",
    "        print('noun')\n",
    "    else:\n",
    "        print('?')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Caption Augmentation_train data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_caption import *\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"input\": 'D:/manuscript test/input/',\n",
    "    \"output\": 'D:/manuscript test/output/',\n",
    "    \"num_aug\": 2,\n",
    "    \"alpha\": 0.1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = None\n",
    "output = args.output\n",
    "num_aug = args.num_aug\n",
    "alpha = args.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "with open('D:/coco text/train_filenames.pickle', 'rb') as f:\n",
    "    train_filenames = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate more data with standard augmentation\n",
    "def gen_eda(train_orig, output_file, alpha, num_aug=9):\n",
    "    writer = open(output_file + 'word based.txt', 'w')\n",
    "    lines = open(train_orig + 'word based.txt', 'r').readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line[:-1].split('\\t')\n",
    "        label = parts[0]\n",
    "        sentence = parts[1]\n",
    "        aug_sentences = eda(sentence, alpha, num_aug)\n",
    "        for aug_sentence in aug_sentences:\n",
    "            writer.write(aug_sentence + '\\n')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #generate augmented sentences and output into a new file\n",
    "    gen_eda(args.input, output, alpha=alpha, num_aug=num_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
